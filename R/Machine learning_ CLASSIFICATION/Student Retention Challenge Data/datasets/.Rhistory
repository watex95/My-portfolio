stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
library(mlbench)
library(caret)
library(caretEnsemble)
# Reload the data and convert the response variable to factor
set.seed(1)
Caravan=as.data.frame(Caravan)
str(Caravan)
Caravan$Purchase<-as.factor(Caravan$Purchase)
caravan.train = Caravan[1:1000,]
caravan.test = Caravan[1001:5822,]
library(mlbench)
library(caret)
library(caretEnsemble)
# Reload the data and convert the response variable to factor
set.seed(1)
Caravan=as.data.frame(Caravan)
dim(Caravan)
Caravan$Purchase<-as.factor(Caravan$Purchase)
caravan.train = Caravan[1:1000,]
caravan.test = Caravan[1001:5822,]
library(mlbench)
library(caret)
library(caretEnsemble)
# Reload the data and convert the response variable to factor
set.seed(1)
Caravan=as.data.frame(Caravan)
Caravan$Purchase<-as.factor(Caravan$Purchase)
caravan.train = Caravan[1:1000,]
caravan.test = Caravan[1001:5822,]
# Example of Stacking algorithms
# create submodels
control<-trainControl(method="repeatedcv", number=10, repeats=3,
savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('rpart','glm', 'knn', 'svmRadial')
models <- caretList(Purchase~.,data=caravan.train,trControl=control
,methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
class(caravan.train$Purchase)
table(caravan.train$Purchase)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(123)
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)
# stack using random forest
set.seed(123)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(123)
stack.glm <- caretStack(models, method="glm", metric="RMSE", trControl=stackControl)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
stack.glm <- caretStack(models, method="glm", metric="rmse", trControl=stackControl)
library(mlbench)
library(caret)
library(caretEnsemble)
set.seed(1)
Caravan=as.data.frame(Caravan)
Caravan$Purchase<-as.factor(Caravan$Purchase)
caravan.train = Caravan[1:1000,]
caravan.test = Caravan[1001:5822,]
# Example of Stacking algorithms
# create submodels
control<-trainControl(method="repeatedcv", number=10, repeats=3,
savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('rpart','glm', 'knn', 'svmRadial')
set.seed(123)
models <- caretList(Purchase~.,data=caravan.train,trControl=control
,methodList=algorithmList))
models<-caretList(Purchase~.,data=caravan.train,trControl=control
,methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
print(stack.glm)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(123)
stack.glm <-caretStack(models, method="glm", metric="rmse", trControl=stackControl)
print(stack.glm)
# correlation between results
modelCor(results)
#We can see that all pairs of predictions have generally low correlation.
#The two methods with the highest correlation between their predictions are Logistic Regression (GLM) and kNN at 0.517 correlation which is not considered high (>0.75).
splom(results)
# stack using random forest
set.seed(123)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
version
updateR()
updateR()
library(installr)
install.packages("installr")
install.packages("installr")
library(installr)
version
updateR()
version
#devtools::install_github("PMassicotte/gtrendsR")
library(gtrendsR)
trends <- gtrends(c("frühling", "spring"), geo = c("DE"))
library(tidyverse)
library(forecast)
library(ggseas)  # install from https://github.com/ellisp/ggseas
install.packages("forecast")
library(tidyverse)
library(forecast)
library(ggseas)  # install from https://github.com/ellisp/ggseas
library(ggseas)  # install from https://github.com/ellisp/ggseas
install.packages("ggseas")
library(tidyverse)
library(forecast)
library(ggseas)  # install from https://github.com/ellisp/ggseas
library(ggseas)  # install from https://github.com/ellisp/ggseas
install.packages("Rtools")
install.Rtools()
install.Rtools(choose_version = TRUE, check = FALSE, GUI = TRUE,
page_with_download_url = "https://cran.r-project.org/bin/windows/Rtools/")
install.packages("Rtools")
install.packages("rtools")
library(Rtools)
# turn dataframe into timeseries
create_ts <- function(df, kw){
df %>%
pluck("interest_over_time") %>%
filter(date    >= as.Date("2014-01-01"),
keyword == kw) %>%
with(ts(hits, start = c(2014, 1), frequency = 52))
}
fruehling  <- create_ts(trends, "frühling")
spring     <- create_ts(trends, "spring")
View(create_ts)
# compare decomposition for both time series
cbind(fruehling, spring) %>%
ggseas::tsdf() %>%
gather("series", "hits", -x) %>%
ggseas::ggsdc(aes(x, hits, colour = series), method = "decompose") +
geom_line() +
labs(x = "time", y ="hits", title = "Decomposition of Search Traffic")
library(Rtools)
install.Rtools()
pkgbuild::has_rtools
function (debug = FALSE)
{
if (!debug && rtools_path_is_set())
return(!identical(rtools_path(), ""))
if (!is_windows())
return(FALSE)
from_config <- scan_config_for_rtools(debug)
if (is_compatible(from_config)) {
if (debug)
cat("Found compatible gcc from R CMD config CC\n")
rtools_path_set(from_config)
return(TRUE)
}
from_path <- scan_path_for_rtools(debug)
if (is_compatible(from_path)) {
if (debug)
cat("Found compatible gcc on path\n")
rtools_path_set(from_path)
return(TRUE)
}
if (!is.null(from_path)) {
if (is.null(from_path$version)) {
if (debug)
cat("gcc and ls on path, assuming set up is correct\n")
return(TRUE)
}
else {
message("WARNING: Rtools ", from_path$version, " found on the path",
" at ", from_path$path, " is not compatible with R ",
getRversion(), ".\n\n", "Please download and install ",
rtools_needed(), " from ", rtools_url, ", remove the incompatible version from your PATH.")
return(invisible(FALSE))
}
}
registry_candidates <- scan_registry_for_rtools(debug)
if (length(registry_candidates) == 0) {
message("WARNING: Rtools is required to build R packages, but is not ",
"currently installed.\n\n", "Please download and install ",
rtools_needed(), " from ", rtools_url, ".")
return(invisible(FALSE))
}
from_registry <- Find(is_compatible, registry_candidates,
right = TRUE)
if (is.null(from_registry)) {
versions <- vapply(registry_candidates, function(x) x$version,
character(1))
message("WARNING: Rtools is required to build R packages, but no version ",
"of Rtools compatible with R ", getRversion(), " was found. ",
"(Only the following incompatible version(s) of Rtools were found:",
paste(versions, collapse = ","), ")\n\n", "Please download and install ",
rtools_needed(), " from ", rtools_url, ".")
return(invisible(FALSE))
}
installed_ver <- installed_version(from_registry$path, debug = debug)
if (is.null(installed_ver)) {
message("WARNING: Rtools is required to build R packages, but the ",
"version of Rtools previously installed in ", from_registry$path,
" has been deleted.\n\n", "Please download and install ",
rtools_needed(), " from ", rtools_url, ".")
return(invisible(FALSE))
}
if (installed_ver != from_registry$version) {
message("WARNING: Rtools is required to build R packages, but no version ",
"of Rtools compatible with R ", getRversion(), " was found. ",
"Rtools ", from_registry$version, " was previously installed in ",
from_registry$path, " but now that directory contains Rtools ",
installed_ver, ".\n\n", "Please download and install ",
rtools_needed(), " from ", rtools_url, ".")
return(invisible(FALSE))
}
rtools_path_set(from_registry)
TRUE
}
install.Rtools(choose_version = TRUE, check = FALSE, GUI = TRUE,
page_with_download_url = "https://cran.r-project.org/bin/windows/Rtools/")
library(devtools)
install.Rtools(choose_version = TRUE, check = FALSE, GUI = TRUE,
page_with_download_url = "https://cran.r-project.org/bin/windows/Rtools/")
find_rtools(T)
Sys.which("ls.exe")
Sys.getenv()['PATH']
Sys.setenv(PATH = paste("C:/Rtools/bin", Sys.getenv("PATH"), sep=";"))
find_rtools(T)4
find_rtools(T)
library(rtools)
library(Rtools)
Sys.which("ls.exe")
Sys.getenv()['PATH']
cbind(fruehling, spring) %>%
ggseas::tsdf() %>%
gather("series", "hits", -x) %>%
ggseas::ggsdc(aes(x, hits, colour = series), method = "decompose") +
geom_line() +
labs(x = "time", y ="hits", title = "Decomposition of Search Traffic")
library(ggseas)  # install from https://github.com/ellisp/ggseas
install.packages("ggseas")
library(ggseas)  # install from https://github.com/ellisp/ggseas
create_ts <- function(df, kw){
df %>%
pluck("interest_over_time") %>%
filter(date    >= as.Date("2014-01-01"),
keyword == kw) %>%
with(ts(hits, start = c(2014, 1), frequency = 52))
}
fruehling  <- create_ts(trends, "frühling")
spring     <- create_ts(trends, "spring")
# compare decomposition for both time series
cbind(fruehling, spring) %>%
ggseas::tsdf() %>%
gather("series", "hits", -x) %>%
ggseas::ggsdc(aes(x, hits, colour = series), method = "decompose") +
geom_line() +
labs(x = "time", y ="hits", title = "Decomposition of Search Traffic")
colnames(financial_aid)
# FINANCIAL AID DATA
financial_aid<-read.csv("financial_aid.csv",header = T)
#Combine by summing the data over the years
library(tidyverse)
attach(financial_aid)
financial_aid=financial_aid %>%
mutate(Total_loan = select(.,c(X2012.Loan,X2013.Loan,X2014.Loan,
X2015.Loan,X2016.Loan,X2017.Loan)) %>%
rowSums(na.rm = TRUE))
financial_aid<-financial_aid%>%
mutate(Total_grant=select(.,c(X2012.Grant,X2013.Grant,X2014.Grant,
X2015.Grant,X2016.Grant,X2017.Grant))%>%
rowSums(na.rm = TRUE))
financial_aid<-financial_aid%>%
mutate(Total_scholarship=select(.,c(X2012.Scholarship,X2013.Scholarship,X2014.Scholarship,
X2015.Scholarship,X2016.Scholarship,X2017.Scholarship))%>%
rowSums(na.rm = TRUE))
financial_aid<-financial_aid%>%
mutate(Total_WorkStudy=select(.,c(X2012.Work.Study,X2013.Work.Study,
X2014.Work.Study,X2015.Work.Study,X2016.Work.Study,X2017.Work.Study))%>%
rowSums(na.rm = TRUE))
setwd("D:/KAZI/UPWORK/KAGGLE COMPETITION/Student Retention Challenge Data/datasets")
# FINANCIAL AID DATA
financial_aid<-read.csv("financial_aid.csv",header = T)
#Combine by summing the data over the years
library(tidyverse)
attach(financial_aid)
financial_aid=financial_aid %>%
mutate(Total_loan = select(.,c(X2012.Loan,X2013.Loan,X2014.Loan,
X2015.Loan,X2016.Loan,X2017.Loan)) %>%
rowSums(na.rm = TRUE))
financial_aid<-financial_aid%>%
mutate(Total_grant=select(.,c(X2012.Grant,X2013.Grant,X2014.Grant,
X2015.Grant,X2016.Grant,X2017.Grant))%>%
rowSums(na.rm = TRUE))
financial_aid<-financial_aid%>%
mutate(Total_scholarship=select(.,c(X2012.Scholarship,X2013.Scholarship,X2014.Scholarship,
X2015.Scholarship,X2016.Scholarship,X2017.Scholarship))%>%
rowSums(na.rm = TRUE))
financial_aid<-financial_aid%>%
mutate(Total_WorkStudy=select(.,c(X2012.Work.Study,X2013.Work.Study,
X2014.Work.Study,X2015.Work.Study,X2016.Work.Study,X2017.Work.Study))%>%
rowSums(na.rm = TRUE))
colnames(financial_aid)
financial_aid<-financial_aid[,-c(9:32)]
colnames(financial_aid)
# Load the train labels and the TestIDs
train_labels=read.csv("DropoutTrainLabels.csv",header = T)
testIDs=read.csv("TestIDs.csv",header = T)
# JOIN the train labels and the financial aid data
library(dplyr)
financial_aid_train=left_join(train_labels,financial_aid,by="StudentID")
colnames(financial_aid_train)
# JOIN the test IDs and the financial aid data
financial_aid_test=left_join(testIDs,financial_aid,by="StudentID")
colnames(financial_aid_test)
#check for duplicated StudentID (TRUE=No duplicates, FALSE=duplicates present)
length(unique(financial_aid_train$StudentID)) == nrow(financial_aid_train)
# STATIC DATA
#STATIC data merged through ETL
#load all files and merge them
myETL=function(mypath){
filenames = list.files(path=mypath, full.names=TRUE)
file_load = function(x){read.csv(file=x,header=T)}
datalist = lapply(filenames, file_load)
data2 = do.call(rbind, lapply(datalist, as.data.frame))
return(data2)
}
# call the function
mergedStatic<-myETL("D:/KAZI/UPWORK/KAGGLE COMPETITION/Student Retention Challenge Data/Student Static Data")
#check for duplicated StudentID (TRUE=No duplicates, FALSE=duplicates present)
length(unique(mergedStatic$StudentID)) == nrow(mergedStatic)
# JOIN the train labels and the static data
library(dplyr)
static_train=left_join(train_labels,mergedStatic,by="StudentID")
# JOIN the test IDs and the static data
static_test=left_join(testIDs,mergedStatic,by="StudentID")
myETL=function(mypath){
filenames = list.files(path=mypath, full.names=TRUE)
file_load = function(x){read.csv(file=x,header=T)}
datalist = lapply(filenames, file_load)
data2 = do.call(rbind, lapply(datalist, as.data.frame))
return(data2)
}
# call the function
mergedProgress<-myETL("D:/KAZI/UPWORK/KAGGLE COMPETITION/Student Retention Challenge Data/Student Progress Data")
colnames(mergedProgress)
#check for duplicated StudentID (TRUE=No duplicates, FALSE=duplicates present)
length(unique(mergedProgress$StudentID)) == nrow(mergedProgress)
library(plyr)
# summarise the data to get rid of duplicated StudentID
prog1=ddply(mergedProgress,.(StudentID),summarize,
CompleteDevMath=mean(CompleteDevMath),CompleteDevEnglish=mean(CompleteDevEnglish),
Major1=mean(Major1),Major2=mean(Major2),Complete1=mean(Complete1),
Complete2=mean(Complete2),CompleteCIP1=mean(CompleteCIP1),
CompleteCIP2=mean(CompleteCIP2),TransferIntent=mean(TransferIntent),
DegreeTypeSought=mean(DegreeTypeSought),TermGPA=mean(CumGPA),
CumGPA=mean(CumGPA),number=length(StudentID))
dim(prog1)
prog1<-prog1[,-14]
dim(prog1)
colnames(prog1)
# JOIN the train labels and the progress data
library(dplyr)
progress_train=left_join(train_labels,prog1,by="StudentID")
# JOIN the test IDs and the progress data
progress_test=left_join(testIDs,prog1,by="StudentID")
dim(progress_train)
# Check to ensure the datasets are of the same sizes before merging them
dim(financial_aid_train)
dim(financial_aid_test)
dim(static_train)
dim(static_test)
dim(progress_train)
dim(progress_test)
# TRAINING DATASET
library(dplyr)
join1<-inner_join(financial_aid_train,static_train,by="StudentID")
TRAIN_DATA=inner_join(join1,progress_train,by="StudentID")
dim(TRAIN_DATA)
colnames(TRAIN_DATA)
# TESTING DATASET
join2<-inner_join(financial_aid_test,static_test,by="StudentID")
TEST_DATA<-inner_join(join2,progress_test,by="StudentID")
dim(TEST_DATA)
colnames(TEST_DATA)
# Clean the train data by removing extra dropout variables (Dropout.y,Dropout)
TRAIN_DATA<-select(TRAIN_DATA,-c("Dropout.y"))
TRAIN_DATA<-select(TRAIN_DATA,-c("Dropout"))
# Rename the droput.x to just dropout for consistency
library(dplyr)
colnames(TRAIN_DATA)[colnames(TRAIN_DATA)=="Dropout.x"] <- "Dropout"
colnames(TRAIN_DATA)
dim(TRAIN_DATA)
dim(TEST_DATA)
str(TRAIN_DATA)
# convert (-1) to NULL in order to facilitate imputation later on
str(TRAIN_DATA)
colnames(TRAIN_DATA)
dim(TRAIN_DATA)
dim(TEST_DATA)
# convert (-1) to NULL in order to facilitate imputation later on
str(TRAIN_DATA)
TRAIN_DATA[TRAIN_DATA=="-1"]<-NA
TRAIN_DATA[TRAIN_DATA==""]<-NA
table(TRAIN_DATA$Marital.Status,exclude = NULL)
table(TRAIN_DATA$Dropout,exclude = NULL)
# convert the theorically categorical variables to  factors
colnames(TRAIN_DATA)
# convert empty spaces and (-1) to NULL in order to facilitate imputation later on
str(TRAIN_DATA)
names=c('DegreeTypeSought','TransferIntent','CompleteDevEnglish','CompleteDevMath',
'GatewayEnglishStatus','GatewayMathStatus','EngPlacement','MathPlacement',
'HighDeg','EnrollmentStatus','FirstGen','HSDip','BirthMonth','BirthYear',
'Gender','Campus','CohortTerm','cohort_term','Dropout')
TRAIN_DATA[,names] <- lapply(TRAIN_DATA[,names] , factor)
str(TRAIN_DATA)
table(TRAIN_DATA$cohort_term)
table(TRAIN_DATA$cohort_term,exclude = NULL)
table(TRAIN_DATA$Dropout,exclude = NULL)
table(TRAIN_DATA$cohort_term,exclude = NULL)
table(TRAIN_DATA$Marital.Status,exclude = NULL)
table(TRAIN_DATA$Mother.s.Highest.Grade.Level)
table(TRAIN_DATA$Mother.s.Highest.Grade.Level,exclude = NULL)
table(TRAIN_DATA$Housing,exclude = NULL)
table(TRAIN_DATA$Cohort)
table(TRAIN_DATA$Cohort,exclude = NULL)
table(TRAIN_DATA$CohortTerm,exclude = NULL)
table(TRAIN_DATA$Campus,exclude = NULL)
table(TRAIN_DATA$Address1)
table(TRAIN_DATA$City)
table(TRAIN_DATA$State)
table(TRAIN_DATA$State,exclude = NULL)
View(TRAIN_DATA)
table(TRAIN_DATA$RegistrationDate)
table(TRAIN_DATA$Gender)
table(TRAIN_DATA$Gender,exclude = NULL)
table(TRAIN_DATA$BirthYear)
table(TRAIN_DATA$Hispanic)
table(TRAIN_DATA$Hispanic,exclude = NULL)
table(TRAIN_DATA$AmericanIndian,exclude = NULL)
table(TRAIN_DATA$Hispanic,exclude = NULL)
table(TRAIN_DATA$Asian,exclude = NULL)
table(TRAIN_DATA$NativeHawaiian,exclude = NULL)
table(TRAIN_DATA$White,exclude = NULL)
table(TRAIN_DATA$TwoOrMoreRace,exclude = NULL)
table(TRAIN_DATA$HSDip,exclude = NULL)
table(TRAIN_DATA$FirstGen)
table(TRAIN_DATA$DualHSSummerEnroll)
table(TRAIN_DATA$EnrollmentStatus)
table(TRAIN_DATA$HighDeg)
table(TRAIN_DATA$HighDeg,exclude = NULL)
table(TRAIN_DATA$NumColCredAttemptTransfer)
table(TRAIN_DATA$NumColCredAttemptTransfer)
table(TRAIN_DATA$CumLoanAtEntry)
table(TRAIN_DATA$MathPlacement,exclude = NULL)
table(TRAIN_DATA$DegreeTypeSought)
table(TRAIN_DATA$EngPlacement)
table(TRAIN_DATA$EngPlacement,exclude = NULL)
table(TRAIN_DATA$GatewayMathStatus)
table(TRAIN_DATA$GatewayMathStatus,TRAIN_DATA$Dropout)
table(TRAIN_DATA$Marital.Status,TRAIN_DATA$Dropout,exclude = NULL)
table(TRAIN_DATA$Mother.s.Highest.Grade.Level,TRAIN_DATA$Dropout,exclude = NULL)
corr=cor(TRAIN_DATA$Total_loan,TRAIN_DATA$Total_grant)
library(ggcorrplot)
ggcorrplot(corr, type = "lower", outline.col = "black",
lab=TRUE,ggtheme = ggplot2::theme_gray,
colors = c("#6D9EC1", "white", "#E46726"))
attach(TRAIN_DATA)
colnames(TRAIN_DATA)
# EXPLORATORY DATA ANALYSIS
# Descriptive statistics
ggplot(data = Total_loan, mapping = aes(x = price, y = ..density..)) +
geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
# EXPLORATORY DATA ANALYSIS
# Descriptive statistics
ggplot(data = TRAIN_DATA,mapping = aes(x = Total_grant, y = ..density..)) +
geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
# EXPLORATORY DATA ANALYSIS
# Descriptive statistics
ggplot(data = TRAIN_DATA,mapping = aes(x = Total_grant, y = ..density..)) +
geom_freqpoly(mapping = aes(colour = Dropout), binwidth = 500)
# EXPLORATORY DATA ANALYSIS
# Descriptive statistics
ggplot(data = TRAIN_DATA, mapping = aes(x = Dropout, y = Total_grant)) +
geom_boxplot()
ggplot(data = TRAIN_DATA) +
geom_point(mapping = aes(x = Total_grant, y =Total_loan))
ggplot(data = TRAIN_DATA, mapping = aes(x = Total_loan, y = Total_grant)) +
geom_boxplot()
ggplot(data = TRAIN_DATA, mapping = aes(x = Marital.Status, y = Total_grant)) +
geom_boxplot()
ggplot(data = TRAIN_DATA, mapping = aes(x = Marital.Status, y = Total_grant)) +
geom_barplot()
ggplot(data = TRAIN_DATA, mapping = aes(x = Marital.Status, y = Total_grant)) +
geom_bar()
ggplot(data = TRAIN_DATA, mapping = aes(x = Marital.Status, y = Total_grant)) +
geom_bar()
ggplot(data = TRAIN_DATA, mapping = aes(x = Marital.Status, y = Total_grant)) +
geom_boxplot()
ggplot(data = TRAIN_DATA) +
geom_point(mapping = aes(x = Total_grant, y =Total_loan))
summarise(Total_grant)
summary(Total_grant)
# convert to long format
df <- reshape2::melt(TRAIN_DATA,id.vars = c("Dropout", "Marital_status", "Gender"),
measure.vars = c("Total_loan", "Total_grant", "Total_WorkStudy", "Total_scholarship")
)
# Load the GGally package
library(GGally)
# Create a scatter plot matrix
vars <- c("Total_loan", "Total_grant", "Total_WorkStudy", "Total_scholarship")
ggpairs(TRAIN_DATA[vars])
